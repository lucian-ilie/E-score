{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KMubAwwP04G"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-XZm7sajbmM",
        "outputId": "b59decfa-17a3-4bd9-eded-f6c4e4771445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting blosum\n",
            "  Downloading blosum-2.0.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: blosum\n",
            "Successfully installed blosum-2.0.2\n",
            "Collecting Bio\n",
            "  Downloading bio-1.6.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.4/279.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.8.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.6.0 biopython-1.81 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
            "Successfully installed accelerate-0.24.1 huggingface-hub-0.17.3 safetensors-0.4.0 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting protein-bert\n",
            "  Downloading protein_bert-1.0.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from protein-bert) (2.14.0)\n",
            "Collecting tensorflow-addons (from protein-bert)\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from protein-bert) (3.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from protein-bert) (4.9.3)\n",
            "Collecting pyfaidx (from protein-bert)\n",
            "  Downloading pyfaidx-0.7.2.2-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (67.7.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (6.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.20.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.14.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->protein-bert)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->protein-bert) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->protein-bert) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->pyfaidx->protein-bert) (3.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->protein-bert) (3.2.2)\n",
            "Installing collected packages: typeguard, tensorflow-addons, pyfaidx, protein-bert\n",
            "Successfully installed protein-bert-1.0.1 pyfaidx-0.7.2.2 tensorflow-addons-0.22.0 typeguard-2.13.3\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n",
            "Collecting biotite\n",
            "  Downloading biotite-0.38.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.12 in /usr/local/lib/python3.10/dist-packages (from biotite) (2.31.0)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from biotite) (1.0.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from biotite) (3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (2023.7.22)\n",
            "Installing collected packages: biotite\n",
            "Successfully installed biotite-0.38.0\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install blosum\n",
        "!pip install Bio\n",
        "!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install protein-bert\n",
        "!pip install biopython biotite\n",
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o3BStrXj6JQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdbc315-c9b8-46e5-993b-5fe8350be7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7d8fdba55960>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "import random\n",
        "from scipy import stats\n",
        "import torch\n",
        "import esm\n",
        "import re\n",
        "import os\n",
        "from Bio import SeqIO\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import pickle\n",
        "import statistics\n",
        "import blosum as bl\n",
        "import time\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "from transformers import AlbertModel, AlbertTokenizer\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import XLNetModel, XLNetTokenizer\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device: {}\".format(device))\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9u86RyHKk_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTuu_8jcuDq_"
      },
      "source": [
        "## ProtT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7VGs9HFct_B9"
      },
      "outputs": [],
      "source": [
        "def ProtT5_initialize():\n",
        "\n",
        "  print(\"ProtT5 Initialize : \")\n",
        "  if device.type == 'cpu':\n",
        "    transformer_link = \"Rostlab/prot_t5_xl_uniref50\"\n",
        "    print(transformer_link)\n",
        "  else:\n",
        "    transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "  print(\"Loading: {}\".format(transformer_link))\n",
        "\n",
        "  ProtT5 = T5EncoderModel.from_pretrained(transformer_link)\n",
        "  ProtT5.full() if device.type =='cpu' else ProtT5.half() # only cast to full-precision if no GPU is available\n",
        "  ProtT5 = ProtT5.to(device)\n",
        "  ProtT5 = ProtT5.eval()\n",
        "  ProtT5_tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
        "\n",
        "  return ProtT5 , ProtT5_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HFSxYPYZuCs1"
      },
      "outputs": [],
      "source": [
        "def get_embs_T5(T5, tokenizer, sequences, n):\n",
        "  sequence_examples = sequences[:n]\n",
        "\n",
        "  # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
        "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
        "\n",
        "  # tokenize sequences and pad up to the longest sequence in the batch\n",
        "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  # generate embeddings\n",
        "  with torch.no_grad():\n",
        "      embedding_repr = T5(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  last_layer_repr = embedding_repr.last_hidden_state\n",
        "  final_embs = []\n",
        "  for i in range(len(last_layer_repr)):\n",
        "    final_embs.append(last_layer_repr[i , :len(sequences[i])])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtP4O4na_FAe"
      },
      "source": [
        "## ESM1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r6o9K45CGpxw"
      },
      "outputs": [],
      "source": [
        "def ESM1b_initialize():\n",
        "  print(\"ESM1b Initialize : \")\n",
        "\n",
        "  # Load ESM-2 model\n",
        "  ESM1b, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  ESM1b.eval()  # disables dropout for deterministic results\n",
        "\n",
        "  return ESM1b, batch_converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5VYaYR2lGuRj"
      },
      "outputs": [],
      "source": [
        "def get_embs_ESM1b(ESM1b, batch_converter, sequences, n):\n",
        "  sequences = sequences[:n]\n",
        "  data = [(\"\" , sequences[0])]\n",
        "\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "  # Extract per-residue representations\n",
        "  with torch.no_grad():\n",
        "      results = ESM1b(batch_tokens, repr_layers=[33], return_contacts= False)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  final_embs = []\n",
        "  for i in range(len(token_representations)):\n",
        "    final_embs.append(token_representations[i][1:-1])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4f4hYnn_J-Y"
      },
      "source": [
        "## ESM2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0-hKY_uJ_RQK"
      },
      "outputs": [],
      "source": [
        "def ESM2_initialize():\n",
        "  print(\"ESM2 Initialize : \")\n",
        "\n",
        "  ESM2, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  ESM2.eval()\n",
        "\n",
        "  return ESM2, batch_converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3uy6XUL_BgXP"
      },
      "outputs": [],
      "source": [
        "def get_embs_ESM2(ESM2, batch_converter, sequences, n):\n",
        "  sequences = sequences[:n]\n",
        "  data = [(\"\" , sequences[0])]\n",
        "\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "  # Extract per-residue representations\n",
        "  with torch.no_grad():\n",
        "      results = ESM2(batch_tokens, repr_layers=[33], return_contacts= False)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  final_embs = []\n",
        "  for i in range(len(token_representations)):\n",
        "    final_embs.append(token_representations[i][1:-1])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0xNnG7IFLuU"
      },
      "source": [
        "## ProtBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yIOMmp3nFR66"
      },
      "outputs": [],
      "source": [
        "def ProtBert_initialize():\n",
        "\n",
        "  print(\"ProtBert Initialize : \")\n",
        "\n",
        "  ProtBert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "  ProtBert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "\n",
        "  ProtBert = ProtBert.to(device)\n",
        "  ProtBert = ProtBert.eval()\n",
        "\n",
        "  return ProtBert, ProtBert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yzg4i4vqFSb6"
      },
      "outputs": [],
      "source": [
        "def get_embs_ProtBert(Bert , tokenizer , sequences , n):\n",
        "  sequence_examples = sequences[:n]\n",
        "\n",
        "  # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
        "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
        "\n",
        "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, pad_to_max_length=True)\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embedding = Bert(input_ids=input_ids,attention_mask=attention_mask)[0]\n",
        "\n",
        "  final_embs = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      seq_emd = embedding[seq_num][1:seq_len-1]\n",
        "      final_embs.append(seq_emd)\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LCdgRFBFWW6"
      },
      "source": [
        "## ProtAlbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tQE-EIVnGRNP"
      },
      "outputs": [],
      "source": [
        "def ProtAlbert_initialize():\n",
        "\n",
        "  print(\"ProtAlbert Initialize : \")\n",
        "\n",
        "  ProtAlbert_tokenizer = AlbertTokenizer.from_pretrained(\"Rostlab/prot_albert\", do_lower_case=False)\n",
        "  ProtAlbert = AlbertModel.from_pretrained(\"Rostlab/prot_albert\")\n",
        "\n",
        "  ProtAlbert = ProtAlbert.to(device)\n",
        "  ProtAlbert = ProtAlbert.eval()\n",
        "\n",
        "  return ProtAlbert, ProtAlbert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QKjAfFMBGKQq"
      },
      "outputs": [],
      "source": [
        "def get_embs_ProtAlbert(Albert, Albert_tokenizer, sequences, n):\n",
        "\n",
        "  sequences = [\" \".join(re.sub(r\"[UZOB]\", \"X\", sequence)) for sequence in sequences]\n",
        "  ids = Albert_tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding = 'longest')\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      embedding = Albert(input_ids = input_ids , attention_mask = attention_mask)[0]\n",
        "\n",
        "  final_embs = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      seq_emd = embedding[seq_num][1 : seq_len - 1]\n",
        "      final_embs.append(seq_emd)\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ctdjBhTGi9z"
      },
      "source": [
        "## XLNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_M2uk9EsGn00"
      },
      "outputs": [],
      "source": [
        "def ProtXLNet_initialize():\n",
        "\n",
        "  print(\"ProtXLNet Initialize : \")\n",
        "\n",
        "  ProtXLNet_tokenizer = XLNetTokenizer.from_pretrained(\"Rostlab/prot_xlnet\" , do_lower_case=False)\n",
        "  ProtXLNet = XLNetModel.from_pretrained(\"Rostlab/prot_xlnet\" , mem_len= 1024)\n",
        "\n",
        "  ProtXLNet = ProtXLNet.to(device)\n",
        "  ProtXLNet = ProtXLNet.eval()\n",
        "\n",
        "  return ProtXLNet, ProtXLNet_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bcwC7KpxGrCr"
      },
      "outputs": [],
      "source": [
        "def get_embs_XLNet(XLNet, XLNet_tokenizer, sequences, n):\n",
        "\n",
        "  sequences = [\" \".join(re.sub(r\"[UZOBX]\" , \"<unk>\", sequence)) for sequence in sequences]\n",
        "  ids = XLNet_tokenizer.batch_encode_plus(sequences, add_special_tokens = True, padding = 'longest')\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = XLNet(input_ids = input_ids , attention_mask = attention_mask)\n",
        "      embedding = output.last_hidden_state\n",
        "\n",
        "  final_embs = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      padded_seq_len = len(attention_mask[seq_num])\n",
        "      seq_emd = embedding[seq_num][padded_seq_len - seq_len : padded_seq_len - 2]\n",
        "      final_embs.append(seq_emd)\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6JenWnHYHo"
      },
      "source": [
        "# Alignment Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTFhk7slsYA1"
      },
      "source": [
        "## Global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r5iNmd9o6Q3V"
      },
      "outputs": [],
      "source": [
        "def affine_global_dp(seq_1, seq_2, g_open, g_ext, scoring = \"ProtT5\", Model = None, Model_tokenizer = None):\n",
        "\n",
        "    MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
        "\n",
        "    #initialize the matrix\n",
        "    m = len(seq_1); n = len(seq_2)\n",
        "    M = np.zeros([m + 1, n + 1])\n",
        "    M[0, 1:] = g_open + g_ext * np.arange(0, n, 1)\n",
        "    M[1:, 0] = g_open + g_ext * np.arange(0, m, 1)\n",
        "    L = np.copy(M); U = np.copy(M)\n",
        "    L[1:,0] = L[1:,0]+g_open; U[0,1:] = U[0,1:]+g_open\n",
        "\n",
        "    #fill up\n",
        "    tracer = np.zeros([np.shape(M)[0],np.shape(M)[1],7])\n",
        "\n",
        "    if scoring == \"ProtT5\":\n",
        "      emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtBert\":\n",
        "      emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtAlbert\":\n",
        "      emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtXLNet\":\n",
        "      emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM1b\":\n",
        "      emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM2\":\n",
        "      emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            l_arr = np.array([M[i , j - 1] + g_open, L[i,j - 1] + g_ext])\n",
        "            L[i,j] = np.max(l_arr)\n",
        "            l_where = l_arr==np.max(l_arr)\n",
        "            u_arr = np.array([M[i - 1,j] + g_open, U[i - 1, j] + g_ext])\n",
        "            U[i,j] = np.max(u_arr)\n",
        "            u_where = u_arr == np.max(u_arr)\n",
        "\n",
        "            if scoring in MODELS_LIST:\n",
        "              sim = cos(torch.tensor(emb1[i - 1] , dtype = torch.float32)\n",
        "              , torch.tensor(emb2[j - 1] , dtype = torch.float32)).item()\n",
        "              m_arr = np.array([M[i - 1, j - 1] + sim , U[i, j] , L[i, j]])\n",
        "            M[i, j] = np.max(m_arr)\n",
        "            m_where = m_arr == np.max(m_arr)\n",
        "            idx = np.hstack([m_where, u_where, l_where])\n",
        "            tracer[i, j, idx] = 1\n",
        "\n",
        "    alignment = []\n",
        "    alignment.append(traceback_g(tracer, seq_1, seq_2, affine=True))\n",
        "    alignment = list(set(map(tuple,alignment)))\n",
        "    score = max(M[-1,-1],L[-1,-1],U[-1,-1])\n",
        "\n",
        "    return score ,alignment\n",
        "\n",
        "\n",
        "def traceback_g(tracer,seq_1,seq_2,mat=None,local=False,affine=True):\n",
        "\n",
        "    #get sequence lengths\n",
        "    m = len(seq_1); n = len(seq_2)\n",
        "\n",
        "    #convert to numpy arrays\n",
        "    x = np.array(list(seq_1),dtype='object')\n",
        "    y = np.array(list(seq_2),dtype='object')\n",
        "    roadmap = 1\n",
        "\n",
        "    if local is False: st = [m+1,n+1]\n",
        "    else:\n",
        "        if roadmap == 0: r = np.random.choice(range(np.size(np.where(mat==np.max(mat))[0])),1)[0] #random maxima\n",
        "        elif roadmap == 1: r = -1 #highroad\n",
        "        elif roadmap == 2: r = 0 #lowroad\n",
        "        st = [(np.where(mat==np.max(mat))[0][r])+1,(np.where(mat==np.max(mat))[1][r])+1]\n",
        "\n",
        "        start_size = ((m-st[0])-(n-st[1])) #how many gaps and for which sequence\n",
        "        start_gap = (['-']*abs(start_size))\n",
        "        if start_size>0:\n",
        "            y=np.append(y,start_gap)\n",
        "        elif start_size<0:\n",
        "            x=np.append(x,start_gap)\n",
        "\n",
        "    st_lv = 0\n",
        "    while ((st[0]>1) & (st[1]>1)):\n",
        "        B = np.zeros([2,2]) #define 2x2 box which specifies which way to move\n",
        "        if affine is True:\n",
        "            Tr = np.zeros([7]) #define a 7x1 Tr array (will store arrows at each step)\n",
        "        else:\n",
        "            Tr = np.zeros([3]) #define a 3x1 Tr array (will store arrows at each step)\n",
        "\n",
        "        if affine is False:\n",
        "            Tr[0] = np.copy(tracer[st[0]-1,st[1]-1,0])\n",
        "            Tr[1] = np.copy(tracer[st[0]-1,st[1]-1,1])\n",
        "            Tr[2] = np.copy(tracer[st[0]-1,st[1]-1,2])\n",
        "        else:\n",
        "            Tr[0] = np.copy(tracer[st[0]-1,st[1]-1,0])\n",
        "            Tr[1] = np.copy(tracer[st[0]-1,st[1]-1,1])\n",
        "            Tr[2] = np.copy(tracer[st[0]-1,st[1]-1,2])\n",
        "            Tr[3] = np.copy(tracer[st[0]-1,st[1]-1,3])\n",
        "            Tr[4] = np.copy(tracer[st[0]-1,st[1]-1,4])\n",
        "            Tr[5] = np.copy(tracer[st[0]-1,st[1]-1,5])\n",
        "            Tr[6] = np.copy(tracer[st[0]-1,st[1]-1,6])\n",
        "\n",
        "        if affine is True: levels = [[2,0,1],[4,3],[6,5]]\n",
        "        else: levels = [[2,0,1]]\n",
        "        for l in levels:\n",
        "            if np.sum(Tr[l])>1:\n",
        "                choose = np.where(Tr[l]==1)[0]\n",
        "                Tr[l] = 0\n",
        "                if roadmap == 0: r = np.random.choice(choose,1)[0] #random turning\n",
        "                elif roadmap == 1: r = choose[-1] #highroad\n",
        "                elif roadmap == 2: r = choose[0] #lowroad\n",
        "                Tr[l[r]] = 1\n",
        "\n",
        "        #level up-down\n",
        "        if ((Tr[0]==1) & (st_lv==0)): #diagonal\n",
        "            B[0,0] = 1\n",
        "\n",
        "        if ((Tr[1]==1) & (st_lv==0)):\n",
        "            if affine is True: st_lv = 1 #level up\n",
        "            else:\n",
        "                B[0,1] = 1\n",
        "\n",
        "        if ((Tr[2]==1) & (st_lv==0)):\n",
        "            if affine is True: st_lv = 2 #level down\n",
        "            else:\n",
        "                B[1,0] = 1\n",
        "\n",
        "        #affine gaps allow for level shifts\n",
        "        if affine is True:\n",
        "            if ((Tr[4]==1) & (st_lv==1)): #move up\n",
        "                B[0,1] = 1\n",
        "\n",
        "            if ((Tr[3]==1) & (st_lv==1)): #move up back to main\n",
        "                st_lv = 0\n",
        "                B[0,1] = 1\n",
        "\n",
        "            if ((Tr[6]==1) & (st_lv==2)): #move left\n",
        "                B[1,0] = 1\n",
        "\n",
        "            if ((Tr[5]==1) & (st_lv==2)): #move left back to main\n",
        "                st_lv = 0\n",
        "                B[1,0] = 1\n",
        "\n",
        "        #movements\n",
        "        if B[0,1]==1: #upward\n",
        "            y = np.insert(y,st[1]-1,'-') #add a gap\n",
        "            st[0] = st[0]-1\n",
        "\n",
        "        if B[1,0]==1: #leftward\n",
        "            x = np.insert(x,st[0]-1,'-') #add a gap\n",
        "            st[1] = st[1]-1\n",
        "\n",
        "        if B[0,0]==1: #diagonal\n",
        "            st[1] = st[1]-1\n",
        "            st[0] = st[0]-1\n",
        "\n",
        "    end_size = (np.size(x)-np.size(y)) #how many gaps and for which sequence\n",
        "    end_gap = (['-']*abs(end_size))\n",
        "    if end_size>0:\n",
        "        y=np.insert(y,0,end_gap)\n",
        "    elif end_size<0:\n",
        "        x=np.insert(x,0,end_gap)\n",
        "\n",
        "    #check no overlapping gaps\n",
        "    x = np.where(((x=='-')&(y=='-')),None,x)\n",
        "    y = np.where((x==None),'',y)\n",
        "    x = np.where((x==None),'',x)\n",
        "\n",
        "    return np.sum(x),np.sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiNZQ2SQJM54"
      },
      "source": [
        "## Global-end-gap-free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MqzYFa7dJTY9"
      },
      "outputs": [],
      "source": [
        "def affine_semi_global_dp(seq_1, seq_2, g_open, g_ext, scoring = \"ProtT5\", Model = None, Model_tokenizer = None):\n",
        "\n",
        "    MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
        "\n",
        "    #initialize the matrix\n",
        "    m = len(seq_1); n = len(seq_2)\n",
        "    M = np.zeros([m+1,n+1])\n",
        "    M[0,1:] = 0 ### NO GAP PENALTY\n",
        "    M[1:,0] = 0\n",
        "    L = np.copy(M); U = np.copy(M)\n",
        "    L[1:,0] = 0; U[0,1:] = 0\n",
        "\n",
        "    #fill up\n",
        "    tracer = np.zeros([np.shape(M)[0],np.shape(M)[1],7])\n",
        "\n",
        "    if scoring == \"ProtT5\":\n",
        "      emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtBert\":\n",
        "      emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtAlbert\":\n",
        "      emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtXLNet\":\n",
        "      emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM1b\":\n",
        "      emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM2\":\n",
        "      emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "      emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "      cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    for i in range(1,m+1):\n",
        "        for j in range(1,n+1):\n",
        "            l_arr = np.array([M[i,j-1]+g_open,L[i,j-1]+g_ext])\n",
        "            L[i,j] = np.max(l_arr)\n",
        "            l_where = l_arr==np.max(l_arr)\n",
        "\n",
        "            u_arr = np.array([M[i-1,j]+g_open,U[i-1,j]+g_ext])\n",
        "            U[i,j] = np.max(u_arr)\n",
        "            u_where = u_arr==np.max(u_arr)\n",
        "\n",
        "            if scoring in MODELS_LIST:\n",
        "              sim = cos(torch.tensor(emb1[i - 1] , dtype = torch.float32)\n",
        "              , torch.tensor(emb2[j - 1] , dtype = torch.float32)).item()\n",
        "              m_arr = np.array([M[i-1,j-1] + sim , U[i,j] , L[i,j]])\n",
        "            M[i,j] = np.max(m_arr)\n",
        "            m_where = m_arr==np.max(m_arr)\n",
        "\n",
        "            idx = np.hstack([m_where,u_where,l_where])\n",
        "            tracer[i,j,idx] = 1\n",
        "\n",
        "    st_loc, score = get_best(m, n, M, L, U)\n",
        "\n",
        "    alignment = []\n",
        "    alignment.append(traceback_sg(tracer,seq_1,seq_2,st_loc, affine = True))\n",
        "    alignment = list(set(map(tuple,alignment)))\n",
        "\n",
        "    return score ,alignment\n",
        "\n",
        "def get_best(m, n, M, L, U):\n",
        "  x = [M, L, U]\n",
        "\n",
        "  sts = []\n",
        "\n",
        "  for mat in x:\n",
        "    t1 = [m + 1, np.where(mat[m, ] == np.max(mat[m, ]))[0][-1] + 1]\n",
        "    t2 = [np.where(mat[:, n] == np.max(mat[:, n]))[0][-1] + 1, n + 1]\n",
        "    if mat[t1[0] - 1, t1[1] - 1] >= mat[t2[0] - 1, t2[1] - 1]:\n",
        "      st = t1\n",
        "    else:\n",
        "      st = t2\n",
        "    sts.append(st)\n",
        "\n",
        "  the_best_start = sts[0]\n",
        "  the_best_score = M[sts[0][0] - 1, sts[0][1] - 1]\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    if x[i][sts[i][0] - 1, sts[i][1] - 1] > the_best_score:\n",
        "      the_best_start = sts[i]\n",
        "      the_best_score = x[i][sts[i][0] - 1, sts[i][1] - 1]\n",
        "\n",
        "  return the_best_start, the_best_score\n",
        "\n",
        "def traceback_sg(tracer, seq_1, seq_2, st = \"\", affine = True):\n",
        "\n",
        "    m = len(seq_1); n = len(seq_2)\n",
        "\n",
        "    # print(m)\n",
        "    # print(n)\n",
        "    #convert to numpy arrays\n",
        "    x = np.array(list(seq_1),dtype='object')\n",
        "    y = np.array(list(seq_2),dtype='object')\n",
        "    roadmap=1\n",
        "    # print(st)\n",
        "\n",
        "    start_size = ((m - st[0]) - (n - st[1])) #how many gaps and for which sequence\n",
        "    start_gap = (['-'] * abs(start_size))\n",
        "    if start_size > 0:\n",
        "        y = np.append(y, start_gap)\n",
        "    elif start_size < 0:\n",
        "        x = np.append(x, start_gap)\n",
        "\n",
        "    st_lv = 0\n",
        "    while ((st[0]>1) & (st[1]>1)):\n",
        "\n",
        "        B = np.zeros([2,2]) #define 2x2 box which specifies which way to move\n",
        "\n",
        "        if affine is True:\n",
        "            Tr = np.zeros([7]) #define a 7x1 Tr array (will store arrows at each step)\n",
        "\n",
        "        if affine is False:\n",
        "            Tr[0] = np.copy(tracer[st[0]-1,st[1]-1,0])\n",
        "            Tr[1] = np.copy(tracer[st[0]-1,st[1]-1,1])\n",
        "            Tr[2] = np.copy(tracer[st[0]-1,st[1]-1,2])\n",
        "\n",
        "        else:\n",
        "            #tracer\n",
        "            Tr[0] = np.copy(tracer[st[0]-1,st[1]-1,0])\n",
        "            Tr[1] = np.copy(tracer[st[0]-1,st[1]-1,1])\n",
        "            Tr[2] = np.copy(tracer[st[0]-1,st[1]-1,2])\n",
        "            Tr[3] = np.copy(tracer[st[0]-1,st[1]-1,3])\n",
        "            Tr[4] = np.copy(tracer[st[0]-1,st[1]-1,4])\n",
        "            Tr[5] = np.copy(tracer[st[0]-1,st[1]-1,5])\n",
        "            Tr[6] = np.copy(tracer[st[0]-1,st[1]-1,6])\n",
        "\n",
        "        #bifurcations\n",
        "        if affine is True: levels = [[2,0,1],[4,3],[6,5]]\n",
        "        else: levels = [[2,0,1]]\n",
        "        for l in levels:\n",
        "            if np.sum(Tr[l])>1:\n",
        "                choose = np.where(Tr[l]==1)[0]\n",
        "                Tr[l] = 0\n",
        "                if roadmap == 0: r = np.random.choice(choose,1)[0]\n",
        "                elif roadmap == 1: r = choose[-1]\n",
        "                elif roadmap == 2: r = choose[0]\n",
        "                Tr[l[r]] = 1\n",
        "\n",
        "        #level up-down\n",
        "        if ((Tr[0]==1) & (st_lv==0)): #diagonal\n",
        "            B[0,0] = 1\n",
        "\n",
        "        if ((Tr[1]==1) & (st_lv==0)):\n",
        "            if affine is True: st_lv = 1 #level up\n",
        "            else:\n",
        "                B[0,1] = 1\n",
        "\n",
        "        if ((Tr[2]==1) & (st_lv==0)):\n",
        "            if affine is True: st_lv = 2 #level down\n",
        "            else:\n",
        "                B[1,0] = 1\n",
        "\n",
        "        #affine gaps allow for level shifts\n",
        "        if affine is True:\n",
        "            if ((Tr[4]==1) & (st_lv==1)): #move up\n",
        "                B[0,1] = 1\n",
        "\n",
        "            if ((Tr[3]==1) & (st_lv==1)): #move up back to main\n",
        "                st_lv = 0\n",
        "                B[0,1] = 1\n",
        "\n",
        "            if ((Tr[6]==1) & (st_lv==2)): #move left\n",
        "                B[1,0] = 1\n",
        "\n",
        "            if ((Tr[5]==1) & (st_lv==2)): #move left back to main\n",
        "                st_lv = 0\n",
        "                B[1,0] = 1\n",
        "\n",
        "        #movements\n",
        "        if B[0,1]==1: #upward\n",
        "            y = np.insert(y,st[1]-1,'-') #add a gap\n",
        "            st[0] = st[0]-1\n",
        "\n",
        "        if B[1,0]==1: #leftward\n",
        "            x = np.insert(x,st[0]-1,'-') #add a gap\n",
        "            st[1] = st[1]-1\n",
        "\n",
        "        if B[0,0]==1: #diagonal\n",
        "            st[1] = st[1]-1\n",
        "            st[0] = st[0]-1\n",
        "\n",
        "    end_size = (np.size(x)-np.size(y)) #how many gaps and for which sequence\n",
        "    end_gap = (['-']*abs(end_size))\n",
        "    if end_size>0:\n",
        "        y=np.insert(y,0,end_gap)\n",
        "    elif end_size<0:\n",
        "        x=np.insert(x,0,end_gap)\n",
        "\n",
        "    #check no overlapping gaps\n",
        "    x = np.where(((x=='-')&(y=='-')),None,x)\n",
        "    y = np.where((x==None),'',y)\n",
        "    x = np.where((x==None),'',x)\n",
        "\n",
        "    return np.sum(x),np.sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IMkkTPWsaF0"
      },
      "source": [
        "# Aux Funx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LBNOVyaN7ZT3"
      },
      "outputs": [],
      "source": [
        "def load_fasta(path):\n",
        "    fasta_sequences = SeqIO.parse(open(path),'fasta')\n",
        "    sequences = []\n",
        "\n",
        "    for fasta in fasta_sequences:\n",
        "        name, sequence = fasta.id, str(fasta.seq)\n",
        "        sequences.append((name, sequence.upper()))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def just_seqs(seqs):\n",
        "    final_seqs = []\n",
        "    for seq in seqs:\n",
        "        final_seqs.append(seq[1])\n",
        "\n",
        "    return final_seqs\n",
        "\n",
        "def permute(prot1,  prot2):\n",
        "    prot1 = [*prot1]\n",
        "    random.shuffle(prot1)\n",
        "    permuted1 = ''.join(prot1)\n",
        "\n",
        "    prot2 = [*prot2]\n",
        "    random.shuffle(prot2)\n",
        "    permuted2 = ''.join(prot2)\n",
        "\n",
        "    return permuted1, permuted2\n",
        "\n",
        "def sort_by_indexes(lst, indexes, reverse=False):\n",
        "  return [val for (_, val) in sorted(zip(indexes, lst), key=lambda x: \\\n",
        "          x[0], reverse=reverse)]\n",
        "\n",
        "def aligned_to_indexed(seqs):\n",
        "  no_dash = []\n",
        "  positions = []\n",
        "  for seq in seqs:\n",
        "    no_dash.append(seq.replace(\"-\" , \"\"))\n",
        "    pos = []\n",
        "    for i , char in enumerate(seq):\n",
        "      if char != \"-\":\n",
        "        pos.append(i)\n",
        "    positions.append(pos)\n",
        "\n",
        "  return no_dash, positions\n",
        "\n",
        "def length_matcher(x , y , place = \"\"):\n",
        "  length = 5\n",
        "\n",
        "  if len(x) < length:\n",
        "    spaces = abs(len(x) - length)\n",
        "\n",
        "    if place == \"Back\":\n",
        "      x = \" \" * spaces + x\n",
        "    if place == \"Front\":\n",
        "      x = x + \" \" * spaces\n",
        "\n",
        "  if len(y) < length:\n",
        "    spaces = abs(len(y) - length)\n",
        "\n",
        "    if place == \"Back\":\n",
        "      y = \" \" * spaces + y\n",
        "    if place == \"Front\":\n",
        "      y = y + \" \" * spaces\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def get_no_gap_indexes(string):\n",
        "  return [(i + 1) for i, s in enumerate(string) if '-' not in s]\n",
        "\n",
        "def excessive_dash_del(x , y):\n",
        "  x_new = \"\"\n",
        "  y_new = \"\"\n",
        "\n",
        "  length = len(x)\n",
        "\n",
        "  for i in range(length):\n",
        "    if x[i] == \"-\" and y[i] == \"-\":\n",
        "      continue\n",
        "    else:\n",
        "      x_new += x[i]\n",
        "      y_new += y[i]\n",
        "\n",
        "  return x_new , y_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achF78kUscys"
      },
      "source": [
        "# Alignment Computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DawofHCVAdVo"
      },
      "outputs": [],
      "source": [
        "def get_alignments(prot1, prot2, gap_penalty = 0, gap_extension_penalty = 0 ,\n",
        "                   scoring = \"\" , alignment_type = \"\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
        "\n",
        "    if alignment_type == \"Global-regular\":\n",
        "      score , alignment= affine_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
        "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
        "\n",
        "    if alignment_type == \"Global-end-gap-free\":\n",
        "      score , alignment= affine_semi_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
        "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
        "\n",
        "    return alignment[0][0], alignment[0][1], score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5Hfj5rAk4_cg"
      },
      "outputs": [],
      "source": [
        "def get_visualization(prot1, prot2 , score , Type = \"\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
        "\n",
        "  MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  seqs = [prot1 , prot2]\n",
        "  no_dash , positions = aligned_to_indexed(seqs)\n",
        "\n",
        "  if Type == \"ProtT5\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_T5(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_T5(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtBert\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_ProtBert(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ProtBert(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtAlbert\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtXLNet\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_XLNet(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_XLNet(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ESM1b\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_ESM1b(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ESM1b(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ESM2\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "\n",
        "    p1_emb = get_embs_ESM2(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ESM2(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  p1_revived = \"\"\n",
        "  p2_revived = \"\"\n",
        "  aligned_info = \"\"\n",
        "\n",
        "  for i in range(len(prot1)):\n",
        "\n",
        "    if i in positions[0]:\n",
        "      p1_revived += prot1[i]\n",
        "    else:\n",
        "      p1_revived += \"-\"\n",
        "\n",
        "    if i in positions[1]:\n",
        "      p2_revived += prot2[i]\n",
        "    else:\n",
        "      p2_revived += \"-\"\n",
        "\n",
        "    if p1_revived[-1] == p2_revived[-1]:\n",
        "      aligned_info += p1_revived[-1]\n",
        "\n",
        "    elif p1_revived[-1] == \"-\" or p2_revived[-1] == \"-\":\n",
        "      aligned_info += \" \"\n",
        "\n",
        "    elif p1_revived[-1] != p2_revived[-1]:\n",
        "\n",
        "      if Type in MODELS_LIST:\n",
        "        sim = cos(torch.tensor(p1_emb[0][positions[0].index(i)] , dtype = torch.float32) ,\n",
        "                  torch.tensor(p2_emb[0][positions[1].index(i)] , dtype = torch.float32)).item()\n",
        "      if sim > 0:\n",
        "          aligned_info += \" \"\n",
        "      else :\n",
        "        aligned_info += \" \"\n",
        "\n",
        "  return p1_revived , aligned_info, p2_revived, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bKtfIjMqC2dl"
      },
      "outputs": [],
      "source": [
        "def alignment_file_TXT(saving_add, seqs_path, scoring, alignment_type,\n",
        "                       gap_penalty, gap_extension_penalty):\n",
        "\n",
        "  seqs = load_fasta(seqs_path)\n",
        "\n",
        "  prot1 = seqs[0][1]\n",
        "  prot2 = seqs[1][1]\n",
        "\n",
        "  name1 = seqs[0][0]\n",
        "  name2 = seqs[1][0]\n",
        "\n",
        "  if scoring == \"ProtT5\":\n",
        "    Model , Model_Tokenizer = ProtT5_initialize()\n",
        "\n",
        "  if scoring == \"ProtBert\":\n",
        "    Model , Model_Tokenizer = ProtBert_initialize()\n",
        "\n",
        "  if scoring == \"ProtAlbert\":\n",
        "    Model , Model_Tokenizer = ProtAlbert_initialize()\n",
        "\n",
        "  if scoring == \"ProtXLNet\":\n",
        "    Model , Model_Tokenizer = ProtXLNet_initialize()\n",
        "\n",
        "  if scoring == \"ESM1b\":\n",
        "    Model , Model_Tokenizer = ESM1b_initialize()\n",
        "\n",
        "  if scoring == \"ESM2\":\n",
        "    Model , Model_Tokenizer = ESM2_initialize()\n",
        "\n",
        "  reference_al, query_al, al_score = get_alignments(prot1, prot2, gap_penalty = gap_penalty,\n",
        "                    gap_extension_penalty = gap_extension_penalty ,\n",
        "                                              scoring = scoring , alignment_type = alignment_type,\n",
        "                                              Model = Model , Model_Tokenizer = Model_Tokenizer)\n",
        "\n",
        "  p1_al , aligned_info , p2_al , al_score = get_visualization(reference_al , query_al, al_score , Type = scoring,\n",
        "                                                              Model = Model, Model_Tokenizer = Model_Tokenizer)\n",
        "\n",
        "  full_al_1 = p1_al\n",
        "  full_al_2 = p2_al\n",
        "\n",
        "  FOLDER = saving_add\n",
        "\n",
        "  if not os.path.exists(FOLDER):\n",
        "   os.makedirs(FOLDER)\n",
        "\n",
        "  file_name = FOLDER + seqs_path.split(\"/\")[-1].split(\".\")[-2] + \"_\" + scoring + \"_\" + alignment_type + \"_\" + \"Alignment_\" + str(gap_penalty) + \"_\" + str(gap_extension_penalty) + \".txt\"\n",
        "  f = open(file_name, \"w\")\n",
        "\n",
        "  f.write(\"Seq 1 \\n\")\n",
        "  f.write(\">\" + name1)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(reference_al.replace(\"-\" , \"\"))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Seq 2 \\n\")\n",
        "  f.write(\">\" + name2)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(query_al.replace(\"-\" , \"\"))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Alignment Type : \" + alignment_type)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  f.write(\"Opening Gap Penalty : \" + str(gap_penalty))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Extension Gap Penalty : \" + str(gap_extension_penalty))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Scoring System : \" + scoring)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Score : \"  + str(al_score))\n",
        "\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  p1_pos = 1\n",
        "  p2_pos = 1\n",
        "  aligned_gaps = \"\"\n",
        "\n",
        "  for j in range(int(len(p1_al) / 60) + 1):\n",
        "    p1_posix = p1_al[j * 60: (j + 1) * 60]\n",
        "    p2_posix = p2_al[j * 60: (j + 1) * 60]\n",
        "\n",
        "    if set(list(p1_posix)) == set(\"-\"):\n",
        "      back1 = p1_pos - 1\n",
        "    else:\n",
        "      back1 = p1_pos\n",
        "    if set(list(p2_posix)) == set(\"-\"):\n",
        "      back2 = p2_pos - 1\n",
        "    else:\n",
        "      back2 = p2_pos\n",
        "\n",
        "    p1_back_str, p2_back_str = length_matcher(str(back1) , str(back2) , place = \"Front\")\n",
        "\n",
        "    for k in range(len(p1_posix)):\n",
        "      if p1_posix[k] != \"-\":\n",
        "        p1_pos += 1\n",
        "      if p2_posix[k] != \"-\":\n",
        "        p2_pos += 1\n",
        "\n",
        "    p1_end_str, p2_end_str = length_matcher(str(p1_pos - 1) , str(p2_pos - 1) , place = \"Back\")\n",
        "    aligned_gaps = \" \" * len(p1_back_str)\n",
        "\n",
        "    f.write(\"Seq 1 : \" + p1_back_str + \" \" + p1_al[j * 60: (j + 1) * 60] + \" \" + p1_end_str)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"        \"  +  aligned_gaps + \" \" + aligned_info[j * 60: (j + 1) * 60])\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"Seq 2 : \"  + p2_back_str + \" \" + p2_al[j * 60: (j + 1) * 60] + \" \" + p2_end_str)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "  print(\"Alignment Computation is Done!\")\n",
        "\n",
        "  del Model\n",
        "  del Model_Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pairwise Alignment Using E-score"
      ],
      "metadata": {
        "id": "Jk7PAnBpD73N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user_guide(MODELS_LIST , ALIGNMENT_TYPES):\n",
        "  print(\"Parameters & Descriptions : \")\n",
        "  print()\n",
        "  print(\"saving_add : the path to the directory for the output\")\n",
        "  print(\"seqs_path : the path of the directory for the FASTA file with two protein sequences\")\n",
        "  print(\"scoring_type : the embedding method used to produce the embedding vectors;\"  +\n",
        "        \" allowed values are: \" , end = \"\")\n",
        "  for model_name in MODELS_LIST[:-1] : print(model_name + \", \" , end = \"\")\n",
        "  print(MODELS_LIST[-1])\n",
        "\n",
        "  print(\"alignment_type: Global-regular or Global-end-gap-free\")\n",
        "  print(\"gap_penalty = -0.25 (default); Recommended Values: -1, -0.5, -0.4, -0.3, -0.2, -0.1\")\n",
        "  print(\"gap_extension_penalty = -0.01 (default); Recommended Values: -0.2, -0.05, -0.04, -0.03, -0.02, -0.01\")\n",
        "\n",
        "\n",
        "MODELS_LIST = [\"ProtT5\" , \"ESM2\" , \"ProtBert\" , \"ProtAlbert\" , \"ESM1b\" ,\"ProtXLNet\"]\n",
        "ALIGNMENT_TYPES = [\"Global-regular\" , \"Global-end-gap-free\"]\n",
        "\n",
        "user_guide(MODELS_LIST , ALIGNMENT_TYPES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwPXU3qNDE39",
        "outputId": "33a1e8ea-a4f7-4c50-aec3-f46d1121adf0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters & Descriptions : \n",
            "\n",
            "saving_add : the path to the directory for the output\n",
            "seqs_path : the path of the directory for the FASTA file with two protein sequences\n",
            "scoring_type : the embedding method used to produce the embedding vectors; allowed values are: ProtT5, ESM2, ProtBert, ProtAlbert, ESM1b, ProtXLNet\n",
            "alignment_type: Global-regular or Global-end-gap-free\n",
            "gap_penalty = -0.25 (default); Recommended Values: -1, -0.5, -0.4, -0.3, -0.2, -0.1\n",
            "gap_extension_penalty = -0.01 (default); Recommended Values: -0.2, -0.05, -0.04, -0.03, -0.02, -0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saving_add =  \"/content/\"\n",
        "seqs_path = \"TestOne.fasta\"\n",
        "scoring = MODELS_LIST[0] # \"ProtT5\"\n",
        "alignment_type = ALIGNMENT_TYPES[0] # \"Global-regular\"\n",
        "gap_penalty = -0.25\n",
        "gap_extension_penalty = -0.01\n",
        "\n",
        "alignment_file_TXT(saving_add = saving_add , seqs_path = seqs_path, scoring = scoring, alignment_type = alignment_type,\n",
        "                      gap_penalty = gap_penalty, gap_extension_penalty = gap_extension_penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoaekbKsJ0DQ",
        "outputId": "a6ff8759-4ecc-46fc-abb0-d81c924c5f4d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProtT5 Initialize : \n",
            "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n",
            "Alignment Computation is Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-3d55d3b91d9f>:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sim = cos(torch.tensor(p1_emb[0][positions[0].index(i)] , dtype = torch.float32) ,\n",
            "<ipython-input-26-3d55d3b91d9f>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(p2_emb[0][positions[1].index(i)] , dtype = torch.float32)).item()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1KMubAwwP04G",
        "Dq9u86RyHKk_",
        "QTuu_8jcuDq_",
        "GtP4O4na_FAe",
        "m4f4hYnn_J-Y",
        "B0xNnG7IFLuU",
        "8LCdgRFBFWW6",
        "qs6JenWnHYHo",
        "iTFhk7slsYA1",
        "wiNZQ2SQJM54",
        "9IMkkTPWsaF0",
        "achF78kUscys"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}