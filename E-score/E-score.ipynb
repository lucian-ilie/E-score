{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KMubAwwP04G"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-XZm7sajbmM",
        "outputId": "bb0721c6-14f5-4d0f-9ee6-ba68d0383d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting blosum\n",
            "  Downloading blosum-2.0.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: blosum\n",
            "Successfully installed blosum-2.0.2\n",
            "Collecting Bio\n",
            "  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.1)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.7.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.23.5)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2023.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.9 biopython-1.81 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, accelerate\n",
            "Successfully installed accelerate-0.22.0 huggingface-hub-0.16.4 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.32.1\n",
            "Collecting protein-bert\n",
            "  Downloading protein_bert-1.0.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from protein-bert) (2.12.0)\n",
            "Collecting tensorflow-addons (from protein-bert)\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from protein-bert) (3.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from protein-bert) (4.9.3)\n",
            "Collecting pyfaidx (from protein-bert)\n",
            "  Downloading pyfaidx-0.7.2.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2023.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.57.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.20.3)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.33.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->protein-bert)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->protein-bert) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->protein-bert) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->protein-bert) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.2.2)\n",
            "Installing collected packages: typeguard, pyfaidx, tensorflow-addons, protein-bert\n",
            "Successfully installed protein-bert-1.0.1 pyfaidx-0.7.2.1 tensorflow-addons-0.21.0 typeguard-2.13.3\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n",
            "Collecting biotite\n",
            "  Downloading biotite-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.12 in /usr/local/lib/python3.10/dist-packages (from biotite) (2.31.0)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from biotite) (1.0.5)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from biotite) (3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.12->biotite) (2023.7.22)\n",
            "Installing collected packages: biotite\n",
            "Successfully installed biotite-0.37.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install blosum\n",
        "!pip install Bio\n",
        "!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install protein-bert\n",
        "!pip install biopython biotite\n",
        "!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3BStrXj6JQ2",
        "outputId": "af5c2e52-88a3-485f-f382-e704171b2969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "import blosum as bl\n",
        "from Bio import SeqIO\n",
        "import random\n",
        "from scipy import stats\n",
        "import torch\n",
        "import esm\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import statistics\n",
        "\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "from transformers import AlbertModel, AlbertTokenizer\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import XLNetModel, XLNetTokenizer\n",
        "import time\n",
        "from zipfile import ZipFile\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device: {}\".format(device))\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9u86RyHKk_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTuu_8jcuDq_"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7VGs9HFct_B9"
      },
      "outputs": [],
      "source": [
        "def ProtT5_initialize():\n",
        "\n",
        "  print(\"ProtT5 Initialize : \")\n",
        "  transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "  print(\"Loading: {}\".format(transformer_link))\n",
        "  T5 = T5EncoderModel.from_pretrained(transformer_link)\n",
        "  T5.full() if device=='cpu' else T5.half() # only cast to full-precision if no GPU is available\n",
        "  T5 = T5.to(device)\n",
        "  T5 = T5.eval()\n",
        "  T5_tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
        "\n",
        "  return T5 , T5_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HFSxYPYZuCs1"
      },
      "outputs": [],
      "source": [
        "def get_embs_T5(T5, tokenizer, sequences, n):\n",
        "  sequence_examples = sequences[:n]\n",
        "\n",
        "  # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
        "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
        "\n",
        "  # tokenize sequences and pad up to the longest sequence in the batch\n",
        "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding= True)\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  # generate embeddings\n",
        "  with torch.no_grad():\n",
        "      embedding_repr = T5(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  last_layer_repr = embedding_repr.last_hidden_state\n",
        "  final_embs = []\n",
        "  for i in range(len(last_layer_repr)):\n",
        "    final_embs.append(last_layer_repr[i , :len(sequences[i])])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtP4O4na_FAe"
      },
      "source": [
        "## ESM1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r6o9K45CGpxw"
      },
      "outputs": [],
      "source": [
        "def ESM1b_initialize():\n",
        "  # Load ESM-1b model\n",
        "  print(\"ESM1b Initialize : \")\n",
        "  ESM1b, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  ESM1b.eval()  # disables dropout for deterministic results\n",
        "\n",
        "  return ESM1b, batch_converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5VYaYR2lGuRj"
      },
      "outputs": [],
      "source": [
        "def get_embs_ESM1b(ESM1b, batch_converter, sequences, n):\n",
        "  sequences = sequences[:n]\n",
        "  data = [(\"\" , sequences[0])]\n",
        "\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "  # Extract per-residue representations\n",
        "  with torch.no_grad():\n",
        "      results = ESM1b(batch_tokens, repr_layers=[33], return_contacts= False)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  final_embs = []\n",
        "  for i in range(len(token_representations)):\n",
        "    final_embs.append(token_representations[i][1:-1])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4f4hYnn_J-Y"
      },
      "source": [
        "## ESM2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0-hKY_uJ_RQK"
      },
      "outputs": [],
      "source": [
        "def ESM2_initialize():\n",
        "  # Load ESM-2 model\n",
        "  print(\"ESM2 Initialize : \")\n",
        "  ESM2, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  ESM2.eval()  # disables dropout for deterministic results\n",
        "\n",
        "  return ESM2, batch_converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3uy6XUL_BgXP"
      },
      "outputs": [],
      "source": [
        "def get_embs_ESM2(ESM2, batch_converter, sequences, n):\n",
        "  sequences = sequences[:n]\n",
        "  data = [(\"\" , sequences[0])]\n",
        "\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "  # Extract per-residue representations\n",
        "  with torch.no_grad():\n",
        "      results = ESM2(batch_tokens, repr_layers=[33], return_contacts= False)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  final_embs = []\n",
        "  for i in range(len(token_representations)):\n",
        "    final_embs.append(token_representations[i][1:-1])\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0xNnG7IFLuU"
      },
      "source": [
        "## Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yIOMmp3nFR66"
      },
      "outputs": [],
      "source": [
        "def ProtBert_initialize():\n",
        "\n",
        "  print(\"ProtBert Initialize : \")\n",
        "\n",
        "  Bert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "  Bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "\n",
        "  Albert = Bert.to(device)\n",
        "  Albert = Bert.eval()\n",
        "\n",
        "  return Bert, Bert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yzg4i4vqFSb6"
      },
      "outputs": [],
      "source": [
        "def get_embs_ProtBert(Bert , tokenizer , sequences , n):\n",
        "  sequence_examples = sequences[:n]\n",
        "\n",
        "  # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
        "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
        "\n",
        "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, pad_to_max_length=True)\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embedding = Bert(input_ids=input_ids,attention_mask=attention_mask)[0]\n",
        "\n",
        "  final_embs = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      seq_emd = embedding[seq_num][1:seq_len-1]\n",
        "      final_embs.append(seq_emd)\n",
        "\n",
        "  return final_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LCdgRFBFWW6"
      },
      "source": [
        "## Albert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tQE-EIVnGRNP"
      },
      "outputs": [],
      "source": [
        "def ProtAlbert_initialize():\n",
        "\n",
        "  print(\"ProtAlbert Initialize : \")\n",
        "\n",
        "  Albert_tokenizer = AlbertTokenizer.from_pretrained(\"Rostlab/prot_albert\", do_lower_case=False)\n",
        "  Albert = AlbertModel.from_pretrained(\"Rostlab/prot_albert\")\n",
        "\n",
        "  Albert = Albert.to(device)\n",
        "  Albert = Albert.eval()\n",
        "\n",
        "  return Albert, Albert_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QKjAfFMBGKQq"
      },
      "outputs": [],
      "source": [
        "def get_embs_ProtAlbert(Albert, Albert_tokenizer, sequences, n):\n",
        "\n",
        "  sequences = [\" \".join(re.sub(r\"[UZOB]\", \"X\", sequence)) for sequence in sequences]\n",
        "  ids = Albert_tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding = 'longest')\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      embedding = Albert(input_ids = input_ids , attention_mask = attention_mask)[0]\n",
        "\n",
        "  features = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      seq_emd = embedding[seq_num][1 : seq_len - 1]\n",
        "      features.append(seq_emd)\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ctdjBhTGi9z"
      },
      "source": [
        "## XLNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_M2uk9EsGn00"
      },
      "outputs": [],
      "source": [
        "def XLNet_initialize():\n",
        "\n",
        "  print(\"ProtXLNet Initialize : \")\n",
        "\n",
        "  XLNet_tokenizer = XLNetTokenizer.from_pretrained(\"Rostlab/prot_xlnet\" , do_lower_case=False)\n",
        "  XLNet = XLNetModel.from_pretrained(\"Rostlab/prot_xlnet\" , mem_len= 1024)\n",
        "\n",
        "  XLNet = XLNet.to(device)\n",
        "  XLNet = XLNet.eval()\n",
        "\n",
        "  return XLNet, XLNet_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bcwC7KpxGrCr"
      },
      "outputs": [],
      "source": [
        "def get_embs_XLNet(XLNet, XLNet_tokenizer, sequences, n):\n",
        "\n",
        "  sequences = [\" \".join(re.sub(r\"[UZOBX]\" , \"<unk>\", sequence)) for sequence in sequences]\n",
        "  ids = XLNet_tokenizer.batch_encode_plus(sequences, add_special_tokens = True, padding = 'longest')\n",
        "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
        "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = XLNet(input_ids = input_ids , attention_mask = attention_mask)\n",
        "      embedding = output.last_hidden_state\n",
        "\n",
        "  features = []\n",
        "  for seq_num in range(len(embedding)):\n",
        "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "      padded_seq_len = len(attention_mask[seq_num])\n",
        "      seq_emd = embedding[seq_num][padded_seq_len - seq_len : padded_seq_len - 2]\n",
        "      features.append(seq_emd)\n",
        "\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6JenWnHYHo"
      },
      "source": [
        "# Alignment Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTFhk7slsYA1"
      },
      "source": [
        "## Global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r5iNmd9o6Q3V"
      },
      "outputs": [],
      "source": [
        "def affine_global_dp(seq_1, seq_2, g_open, g_ext,\n",
        "                     scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
        "\n",
        "    MODELS_LIST = [\"ProtT5\", \"ProtBert\", \"ProtAlbert\", \"ProtXLNet\", \"ESM1b\", \"ESM2\"]\n",
        "\n",
        "    # initialize the matrix\n",
        "    m = len(seq_1);\n",
        "    n = len(seq_2)\n",
        "    M = np.zeros([m + 1, n + 1])\n",
        "    M[0, 1:] = g_open + g_ext * np.arange(0, n, 1)\n",
        "    M[1:, 0] = g_open + g_ext * np.arange(0, m, 1)\n",
        "    L = np.copy(M);\n",
        "    U = np.copy(M)\n",
        "    L[1:, 0] = L[1:, 0] + g_open;\n",
        "    U[0, 1:] = U[0, 1:] + g_open  # avoiding Gotoh's error\n",
        "\n",
        "    # fill up\n",
        "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
        "\n",
        "    if scoring == \"ProtT5\":\n",
        "        emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtBert\":\n",
        "        emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtAlbert\":\n",
        "        emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtXLNet\":\n",
        "        emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM1b\":\n",
        "        emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM2\":\n",
        "        emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
        "            L[i, j] = np.max(l_arr)\n",
        "            l_where = l_arr == np.max(l_arr)\n",
        "\n",
        "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
        "            U[i, j] = np.max(u_arr)\n",
        "            u_where = u_arr == np.max(u_arr)\n",
        "\n",
        "            if scoring in MODELS_LIST:\n",
        "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32)\n",
        "                          , torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
        "\n",
        "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
        "\n",
        "            M[i, j] = np.max(m_arr)\n",
        "            m_where = m_arr == np.max(m_arr)\n",
        "\n",
        "            idx = np.hstack([m_where, u_where, l_where])\n",
        "            tracer[i, j, idx] = 1\n",
        "\n",
        "    # traceback\n",
        "\n",
        "    alignment = []\n",
        "    alignment.append(traceback_g(tracer, seq_1, seq_2, affine= True, roadmap=0))\n",
        "\n",
        "    alignment = list(set(map(tuple, alignment)))\n",
        "\n",
        "    return M, L, U, tracer, alignment\n",
        "\n",
        "\n",
        "def traceback_g(tracer, seq_1, seq_2, mat=None, affine=False, roadmap=0):\n",
        "    # get sequence lengths\n",
        "    m = len(seq_1);\n",
        "    n = len(seq_2)\n",
        "\n",
        "    # convert to numpy arrays\n",
        "    x = np.array(list(seq_1), dtype='object')\n",
        "    y = np.array(list(seq_2), dtype='object')\n",
        "\n",
        "    # set start location\n",
        "    st = [m + 1, n + 1]\n",
        "\n",
        "    st_lv = 0  # start in midgard\n",
        "\n",
        "    while ((st[0] > 1) & (st[1] > 1)):\n",
        "\n",
        "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
        "\n",
        "        if affine is True:\n",
        "            Tr = np.zeros([7])  # define a 7x1 Tr array (will store arrows at each step)\n",
        "        else:\n",
        "            Tr = np.zeros([3])  # define a 3x1 Tr array (will store arrows at each step)\n",
        "\n",
        "        if affine is False:\n",
        "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
        "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
        "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
        "\n",
        "        else:\n",
        "            # tracer\n",
        "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
        "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
        "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
        "            Tr[3] = np.copy(tracer[st[0] - 1, st[1] - 1, 3])\n",
        "            Tr[4] = np.copy(tracer[st[0] - 1, st[1] - 1, 4])\n",
        "            Tr[5] = np.copy(tracer[st[0] - 1, st[1] - 1, 5])\n",
        "            Tr[6] = np.copy(tracer[st[0] - 1, st[1] - 1, 6])\n",
        "\n",
        "        # bifurcations\n",
        "        if affine is True:\n",
        "            levels = [[2, 0, 1], [4, 3], [6, 5]]\n",
        "        else:\n",
        "            levels = [[2, 0, 1]]\n",
        "        for l in levels:\n",
        "            if np.sum(Tr[l]) > 1:\n",
        "                choose = np.where(Tr[l] == 1)[0]\n",
        "                Tr[l] = 0\n",
        "                if roadmap == 0:\n",
        "                    r = np.random.choice(choose, 1)[0]  # random turning\n",
        "                elif roadmap == 1:\n",
        "                    r = choose[-1]  # highroad\n",
        "                elif roadmap == 2:\n",
        "                    r = choose[0]  # lowroad\n",
        "                else:\n",
        "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
        "                Tr[l[r]] = 1\n",
        "\n",
        "        # level up-down\n",
        "        if ((Tr[0] == 1) & (st_lv == 0)):  # diagonal\n",
        "            B[0, 0] = 1\n",
        "\n",
        "        if ((Tr[1] == 1) & (st_lv == 0)):\n",
        "            if affine is True:\n",
        "                st_lv = 1  # level up\n",
        "            else:\n",
        "                B[0, 1] = 1\n",
        "\n",
        "        if ((Tr[2] == 1) & (st_lv == 0)):\n",
        "            if affine is True:\n",
        "                st_lv = 2  # level down\n",
        "            else:\n",
        "                B[1, 0] = 1\n",
        "\n",
        "        # affine gaps allow for level shifts\n",
        "        if affine is True:\n",
        "            if ((Tr[4] == 1) & (st_lv == 1)):  # move up\n",
        "                B[0, 1] = 1\n",
        "\n",
        "            if ((Tr[3] == 1) & (st_lv == 1)):  # move up back to main\n",
        "                st_lv = 0\n",
        "                B[0, 1] = 1\n",
        "\n",
        "            if ((Tr[6] == 1) & (st_lv == 2)):  # move left\n",
        "                B[1, 0] = 1\n",
        "\n",
        "            if ((Tr[5] == 1) & (st_lv == 2)):  # move left back to main\n",
        "                st_lv = 0\n",
        "                B[1, 0] = 1\n",
        "\n",
        "        # movements\n",
        "        if B[0, 1] == 1:  # upward\n",
        "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
        "            st[0] = st[0] - 1\n",
        "\n",
        "        if B[1, 0] == 1:  # leftward\n",
        "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
        "            st[1] = st[1] - 1\n",
        "\n",
        "        if B[0, 0] == 1:  # diagonal\n",
        "            st[1] = st[1] - 1\n",
        "            st[0] = st[0] - 1\n",
        "\n",
        "    # some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
        "    end_size = (np.size(x) - np.size(y))  # how many gaps and for which sequence\n",
        "    end_gap = (['-'] * abs(end_size))\n",
        "    if end_size > 0:\n",
        "        y = np.insert(y, 0, end_gap)\n",
        "    elif end_size < 0:\n",
        "        x = np.insert(x, 0, end_gap)\n",
        "\n",
        "    # check no overlapping gaps\n",
        "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
        "    y = np.where((x == None), '', y)\n",
        "    x = np.where((x == None), '', x)\n",
        "\n",
        "    return np.sum(x), np.sum(y)\n",
        "\n",
        "\n",
        "def traceback_iterator_g(tracer, seq_1, seq_2,\n",
        "                         affine=False):\n",
        "    alignment = []\n",
        "    alignment.append(traceback_g(tracer, seq_1, seq_2, affine=affine, roadmap=0))\n",
        "\n",
        "    return list(set(map(tuple, alignment)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiNZQ2SQJM54"
      },
      "source": [
        "## Prefix/Suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MqzYFa7dJTY9"
      },
      "outputs": [],
      "source": [
        "def affine_semi_global_dp(seq_1, seq_2, g_open, g_ext,\n",
        "                          high_low=False, scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
        "    MODELS_LIST = [\"ProtT5\", \"ProtBert\", \"ProtAlbert\", \"ProtXLNet\", \"ESM1b\", \"ESM2\"]\n",
        "\n",
        "    # initialize the matrix\n",
        "    m = len(seq_1);\n",
        "    n = len(seq_2)\n",
        "    M = np.zeros([m + 1, n + 1])\n",
        "    M[0, 1:] = 0\n",
        "    M[1:, 0] = 0\n",
        "    L = np.copy(M);\n",
        "    U = np.copy(M)\n",
        "    L[1:, 0] = 0;\n",
        "    U[0, 1:] = 0\n",
        "\n",
        "    # fill up\n",
        "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
        "\n",
        "    if scoring == \"ProtT5\":\n",
        "        emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtBert\":\n",
        "        emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtAlbert\":\n",
        "        emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ProtXLNet\":\n",
        "        emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM1b\":\n",
        "        emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    if scoring == \"ESM2\":\n",
        "        emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
        "        emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
        "            L[i, j] = np.max(l_arr)\n",
        "            l_where = l_arr == np.max(l_arr)\n",
        "\n",
        "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
        "            U[i, j] = np.max(u_arr)\n",
        "            u_where = u_arr == np.max(u_arr)\n",
        "\n",
        "            if scoring in MODELS_LIST:\n",
        "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32)\n",
        "                          , torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
        "\n",
        "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
        "\n",
        "            M[i, j] = np.max(m_arr)\n",
        "            m_where = m_arr == np.max(m_arr)\n",
        "\n",
        "            idx = np.hstack([m_where, u_where, l_where])\n",
        "            tracer[i, j, idx] = 1\n",
        "\n",
        "\n",
        "    alignment = []\n",
        "    alignment.append(traceback_sg(tracer, seq_1, seq_2, mat=M, affine=True,\n",
        "                                  local= True, roadmap=0))\n",
        "    alignment = list(set(map(tuple, alignment)))\n",
        "\n",
        "    return M, L, U, tracer, alignment\n",
        "\n",
        "\n",
        "def traceback_sg(tracer, seq_1, seq_2, mat=None, local=False, affine=False, roadmap=0):\n",
        "\n",
        "    m = len(seq_1);\n",
        "    n = len(seq_2)\n",
        "\n",
        "    x = np.array(list(seq_1), dtype='object')\n",
        "    y = np.array(list(seq_2), dtype='object')\n",
        "\n",
        "    # set start location\n",
        "    if roadmap == 0:\n",
        "        r = np.random.choice(range(np.size(np.where(mat == np.max(mat))[0])), 1)[0]  # random maxima\n",
        "    elif roadmap == 1:\n",
        "        r = -1\n",
        "    elif roadmap == 2:\n",
        "        r = 0\n",
        "\n",
        "    st = [(np.where(mat == np.max(mat))[0][r]) + 1, (np.where(mat == np.max(mat))[1][r]) + 1]\n",
        "\n",
        "    # set starting gaps based on the start location\n",
        "    start_size = ((m - st[0]) - (n - st[1]))  # how many gaps and for which sequence\n",
        "    start_gap = (['-'] * abs(start_size))\n",
        "    if start_size > 0:\n",
        "        y = np.append(y, start_gap)\n",
        "    elif start_size < 0:\n",
        "        x = np.append(x, start_gap)\n",
        "\n",
        "    st_lv = 0  # start in midgard\n",
        "\n",
        "    while ((st[0] > 1) & (st[1] > 1)):\n",
        "\n",
        "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
        "\n",
        "        if affine is True:\n",
        "            Tr = np.zeros([7])  # define a 7x1 Tr array (will store arrows at each step)\n",
        "        else:\n",
        "            Tr = np.zeros([3])  # define a 3x1 Tr array (will store arrows at each step)\n",
        "\n",
        "\n",
        "        if affine is False:\n",
        "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
        "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
        "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
        "\n",
        "        else:\n",
        "            # tracer\n",
        "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
        "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
        "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
        "            Tr[3] = np.copy(tracer[st[0] - 1, st[1] - 1, 3])\n",
        "            Tr[4] = np.copy(tracer[st[0] - 1, st[1] - 1, 4])\n",
        "            Tr[5] = np.copy(tracer[st[0] - 1, st[1] - 1, 5])\n",
        "            Tr[6] = np.copy(tracer[st[0] - 1, st[1] - 1, 6])\n",
        "\n",
        "        # bifurcations\n",
        "        if affine is True:\n",
        "            levels = [[2, 0, 1], [4, 3], [6, 5]]\n",
        "        else:\n",
        "            levels = [[2, 0, 1]]\n",
        "        for l in levels:\n",
        "            if np.sum(Tr[l]) > 1:\n",
        "                choose = np.where(Tr[l] == 1)[0]\n",
        "                Tr[l] = 0\n",
        "                if roadmap == 0:\n",
        "                    r = np.random.choice(choose, 1)[0]  # random turning\n",
        "                elif roadmap == 1:\n",
        "                    r = choose[-1]  # highroad\n",
        "                elif roadmap == 2:\n",
        "                    r = choose[0]  # lowroad\n",
        "                else:\n",
        "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
        "                Tr[l[r]] = 1\n",
        "\n",
        "        # level up-down\n",
        "        if ((Tr[0] == 1) & (st_lv == 0)):  # diagonal\n",
        "            B[0, 0] = 1\n",
        "\n",
        "        if ((Tr[1] == 1) & (st_lv == 0)):\n",
        "            if affine is True:\n",
        "                st_lv = 1  # level up\n",
        "            else:\n",
        "                B[0, 1] = 1\n",
        "\n",
        "        if ((Tr[2] == 1) & (st_lv == 0)):\n",
        "            if affine is True:\n",
        "                st_lv = 2  # level down\n",
        "            else:\n",
        "                B[1, 0] = 1\n",
        "\n",
        "        # affine gaps allow for level shifts\n",
        "        if affine is True:\n",
        "            if ((Tr[4] == 1) & (st_lv == 1)):  # move up\n",
        "                B[0, 1] = 1\n",
        "\n",
        "            if ((Tr[3] == 1) & (st_lv == 1)):  # move up back to main\n",
        "                st_lv = 0\n",
        "                B[0, 1] = 1\n",
        "\n",
        "            if ((Tr[6] == 1) & (st_lv == 2)):  # move left\n",
        "                B[1, 0] = 1\n",
        "\n",
        "            if ((Tr[5] == 1) & (st_lv == 2)):  # move left back to main\n",
        "                st_lv = 0\n",
        "                B[1, 0] = 1\n",
        "\n",
        "        if local is True:\n",
        "            if (mat[st[0] - 1, st[1] - 1] == 0):\n",
        "                break\n",
        "\n",
        "        # movements\n",
        "        if B[0, 1] == 1:  # upward\n",
        "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
        "            st[0] = st[0] - 1\n",
        "\n",
        "        if B[1, 0] == 1:  # leftward\n",
        "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
        "            st[1] = st[1] - 1\n",
        "\n",
        "        if B[0, 0] == 1:  # diagonal\n",
        "            st[1] = st[1] - 1\n",
        "            st[0] = st[0] - 1\n",
        "\n",
        "    # some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
        "    end_size = (np.size(x) - np.size(y))  # how many gaps and for which sequence\n",
        "    end_gap = (['-'] * abs(end_size))\n",
        "    if end_size > 0:\n",
        "        y = np.insert(y, 0, end_gap)\n",
        "    elif end_size < 0:\n",
        "        x = np.insert(x, 0, end_gap)\n",
        "\n",
        "    # check no overlapping gaps\n",
        "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
        "    y = np.where((x == None), '', y)\n",
        "    x = np.where((x == None), '', x)\n",
        "\n",
        "    return np.sum(x), np.sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IMkkTPWsaF0"
      },
      "source": [
        "# Aux Funx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LBNOVyaN7ZT3"
      },
      "outputs": [],
      "source": [
        "def load_fasta(path):\n",
        "    fasta_sequences = SeqIO.parse(open(path),'fasta')\n",
        "    sequences = []\n",
        "\n",
        "    for fasta in fasta_sequences:\n",
        "        name, sequence = fasta.id, str(fasta.seq)\n",
        "        sequences.append((name, sequence.upper()))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def just_seqs(seqs):\n",
        "    final_seqs = []\n",
        "    for seq in seqs:\n",
        "        final_seqs.append(seq[1])\n",
        "\n",
        "    return final_seqs\n",
        "\n",
        "def aligned_to_indexed(seqs):\n",
        "  no_dash = []\n",
        "  positions = []\n",
        "  for seq in seqs:\n",
        "    no_dash.append(seq.replace(\"-\" , \"\"))\n",
        "    pos = []\n",
        "    for i , char in enumerate(seq):\n",
        "      if char != \"-\":\n",
        "        pos.append(i)\n",
        "    positions.append(pos)\n",
        "\n",
        "  return no_dash, positions\n",
        "\n",
        "def length_matcher(x , y , place = \"\"):\n",
        "  length = 5\n",
        "\n",
        "  if len(x) < length:\n",
        "    spaces = abs(len(x) - length)\n",
        "\n",
        "    if place == \"Back\":\n",
        "      x = \" \" * spaces + x\n",
        "    if place == \"Front\":\n",
        "      x = x + \" \" * spaces\n",
        "\n",
        "  if len(y) < length:\n",
        "    spaces = abs(len(y) - length)\n",
        "\n",
        "    if place == \"Back\":\n",
        "      y = \" \" * spaces + y\n",
        "    if place == \"Front\":\n",
        "      y = y + \" \" * spaces\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achF78kUscys"
      },
      "source": [
        "# Alignment Computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DawofHCVAdVo"
      },
      "outputs": [],
      "source": [
        "def get_alignments(prot1, prot2, gap_penalty = 0, gap_extension_penalty = 0 ,\n",
        "                   scoring = \"ProtT5\" , alignment_type = \"Global-regular\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
        "\n",
        "    if alignment_type == \"Global-regular\":\n",
        "      M, L, U , tracer , alignment= affine_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
        "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
        "      max_score = np.max(M)\n",
        "\n",
        "    if alignment_type == \"Global-end-gap-free\" or alignment_type == \"End-Gap-Free\":\n",
        "      M, L, U , tracer , alignment= affine_semi_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
        "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
        "\n",
        "      max_score = max(M[-1,-1],L[-1,-1],U[-1,-1])\n",
        "\n",
        "    aligned1 = alignment[0][0]\n",
        "    aligned2 = alignment[0][1]\n",
        "\n",
        "    return aligned1, aligned2, max_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5Hfj5rAk4_cg"
      },
      "outputs": [],
      "source": [
        "def get_visualization(prot1, prot2 , score , Type = \"\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
        "\n",
        "  MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
        "  cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "  seqs = [prot1 , prot2]\n",
        "  no_dash , positions = aligned_to_indexed(seqs)\n",
        "\n",
        "  if Type == \"ProtT5\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_T5(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_T5(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtBert\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_ProtBert(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ProtBert(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtAlbert\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ProtXLNet\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_XLNet(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_XLNet(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ESM1b\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_ESM1b(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ESM1b(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  if Type == \"ESM2\":\n",
        "    model = Model\n",
        "    tokenizer = Model_Tokenizer\n",
        "    p1_emb = get_embs_ESM2(model, tokenizer, [no_dash[0]] , 1)\n",
        "    p2_emb = get_embs_ESM2(model, tokenizer, [no_dash[1]] , 1)\n",
        "\n",
        "  p1_revived = \"\"\n",
        "  p2_revived = \"\"\n",
        "  aligned_info = \"\"\n",
        "\n",
        "  for i in range(len(prot1)):\n",
        "\n",
        "    if i in positions[0]:\n",
        "      p1_revived += prot1[i]\n",
        "    else:\n",
        "      p1_revived += \"-\"\n",
        "\n",
        "    if i in positions[1]:\n",
        "      p2_revived += prot2[i]\n",
        "    else:\n",
        "      p2_revived += \"-\"\n",
        "\n",
        "\n",
        "    if p1_revived[-1] == p2_revived[-1]:\n",
        "      aligned_info += p1_revived[-1]\n",
        "\n",
        "    elif p1_revived[-1] == \"-\" or p2_revived[-1] == \"-\":\n",
        "      aligned_info += \" \"\n",
        "\n",
        "    elif p1_revived[-1] != p2_revived[-1]:\n",
        "\n",
        "      if Type in MODELS_LIST:\n",
        "        sim = cos(torch.tensor(p1_emb[0][positions[0].index(i)] , dtype = torch.float32) ,\n",
        "                  torch.tensor(p2_emb[0][positions[1].index(i)] , dtype = torch.float32)).item()\n",
        "\n",
        "        aligned_info += \" \"\n",
        "\n",
        "  del model\n",
        "  del tokenizer\n",
        "\n",
        "  return p1_revived , aligned_info, p2_revived, score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Alignment For 2 Sequences"
      ],
      "metadata": {
        "id": "m7nB5_MdnBO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alignment_file_TXT(saving_add, seqs_path, scoring, alignment_type,\n",
        "                      gap_penalty, gap_extension_penalty):\n",
        "\n",
        "  if scoring == \"ProtT5\":\n",
        "    Model , Model_Tokenizer = ProtT5_initialize()\n",
        "\n",
        "  if scoring == \"ProtBert\":\n",
        "    Model , Model_Tokenizer = ProtBert_initialize()\n",
        "\n",
        "  if scoring == \"ProtAlbert\":\n",
        "    Model , Model_Tokenizer = ProtAlbert_initialize()\n",
        "\n",
        "  if scoring == \"ProtXLNet\":\n",
        "    Model , Model_Tokenizer = XLNet_initialize()\n",
        "\n",
        "  if scoring == \"ESM1b\":\n",
        "    Model , Model_Tokenizer = ESM1b_initialize()\n",
        "\n",
        "  if scoring == \"ESM2\":\n",
        "    Model , Model_Tokenizer = ESM2_initialize()\n",
        "\n",
        "  seqs = load_fasta(seqs_path)\n",
        "\n",
        "  prot1 = seqs[0][1]\n",
        "  prot2 = seqs[1][1]\n",
        "\n",
        "  name1 = seqs[0][0]\n",
        "  name2 = seqs[1][0]\n",
        "\n",
        "  reference_al, query_al, al_score = get_alignments(prot1, prot2, gap_penalty = gap_penalty,\n",
        "                    gap_extension_penalty = gap_extension_penalty ,\n",
        "                                              scoring = scoring , alignment_type = alignment_type,\n",
        "                                              Model = Model , Model_Tokenizer = Model_Tokenizer)\n",
        "\n",
        "  p1_al , aligned_info , p2_al , al_score = get_visualization(reference_al , query_al, al_score , Type = scoring,\n",
        "                                                              Model = Model, Model_Tokenizer = Model_Tokenizer)\n",
        "\n",
        "  full_al_1 = p1_al\n",
        "  full_al_2 = p2_al\n",
        "\n",
        "  FOLDER = saving_add\n",
        "\n",
        "  if not os.path.exists(FOLDER):\n",
        "   os.makedirs(FOLDER)\n",
        "\n",
        "  file_name = FOLDER + seqs_path.split(\"/\")[-1].split(\".\")[-2] + \"_\" + scoring + \"_\" + alignment_type + \"_\"\n",
        "  file_name += str(gap_penalty) + \"_\" + str(gap_extension_penalty) + \"_\"+ \"Alignment\" + \".txt\"\n",
        "  f = open(file_name, \"w\")\n",
        "\n",
        "  f.write(\"Seq 1 \\n\")\n",
        "  f.write(\">\" + name1)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(reference_al.replace(\"-\" , \"\"))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Seq 2 \\n\")\n",
        "  f.write(\">\" + name2)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(query_al.replace(\"-\" , \"\"))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Alignment Type : \" + alignment_type)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  f.write(\"Opening Gap Penalty : \" + str(gap_penalty))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Extension Gap Penalty : \" + str(gap_extension_penalty))\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Scoring System : \" + scoring)\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"Score : \"  + str(al_score))\n",
        "\n",
        "  f.write(\"\\n\")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  p1_pos = 1\n",
        "  p2_pos = 1\n",
        "  aligned_gaps = \"\"\n",
        "\n",
        "  for j in range(int(len(p1_al) / 60) + 1):\n",
        "    p1_posix = p1_al[j * 60: (j + 1) * 60]\n",
        "    p2_posix = p2_al[j * 60: (j + 1) * 60]\n",
        "    p1_back_str, p2_back_str = length_matcher(str(p1_pos) , str(p2_pos) , place = \"Front\")\n",
        "\n",
        "    for k in range(len(p1_posix)):\n",
        "      if p1_posix[k] != \"-\":\n",
        "        p1_pos += 1\n",
        "      if p2_posix[k] != \"-\":\n",
        "        p2_pos += 1\n",
        "\n",
        "    p1_end_str, p2_end_str = length_matcher(str(p1_pos - 1) , str(p2_pos - 1) , place = \"Back\")\n",
        "    aligned_gaps = \" \" * len(p1_back_str)\n",
        "\n",
        "    f.write(\"Seq 1 : \" + p1_back_str + \" \" + p1_al[j * 60: (j + 1) * 60] + \" \" + p1_end_str)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"        \"  +  aligned_gaps + \" \" + aligned_info[j * 60: (j + 1) * 60])\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"Seq 2 : \"  + p2_back_str + \" \" + p2_al[j * 60: (j + 1) * 60] + \" \" + p2_end_str)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "  print(\"Alignment Computation is Done!\")\n",
        "\n",
        "  del Model\n",
        "  del Model_Tokenizer"
      ],
      "metadata": {
        "id": "dyAQgxCFn99V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_guide(MODELS_LIST , ALIGNMENT_TYPES):\n",
        "  print(\"Parameters & Descriptions : \")\n",
        "  print()\n",
        "  print(\"saving_add : the path to the directory for the output\")\n",
        "  print(\"seqs_path : the path of the directory for the FASTA file with two protein sequences\")\n",
        "  print(\"scoring_type : the embedding method used to produce the embedding vectors;\"  +\n",
        "        \" allowed values are: \" , end = \"\")\n",
        "  for model_name in MODELS_LIST[:-1] : print(model_name + \", \" , end = \"\")\n",
        "  print(MODELS_LIST[-1])\n",
        "\n",
        "  print(\"alignment_type: Global-regular or Global-end-gap-free\")\n",
        "  print(\"gap_penalty = -1 (default); Recommended Values: -4, -3, -2, -1.5, -1, -0.5\")\n",
        "  print(\"gap_extension_penalty = -0.2 (default); Recommended Values: -1, -0.8, -0.5, -0.3, -0.2, -0.1\")"
      ],
      "metadata": {
        "id": "OaiQRMPjkWbg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "AVRUmBYkG4M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_LIST = [\"ProtT5\" , \"ESM2\" , \"ProtBert\" , \"ProtAlbert\" , \"ESM1b\" ,\"ProtXLNet\"]\n",
        "ALIGNMENT_TYPES = [\"Global-regular\" , \"Global-end-gap-free\"]\n",
        "\n",
        "user_guide(MODELS_LIST , ALIGNMENT_TYPES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tyat_ydz5f4",
        "outputId": "d0b958ab-ed49-469e-ae38-44c0c744c10a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters & Descriptions : \n",
            "\n",
            "saving_add : the path to the directory for the output\n",
            "seqs_path : the path of the directory for the FASTA file with two protein sequences\n",
            "scoring_type : the embedding method used to produce the embedding vectors; allowed values are: ProtT5, ESM2, ProtBert, ProtAlbert, ESM1b, ProtXLNet\n",
            "alignment_type: Global-regular or Global-end-gap-free\n",
            "gap_penalty = -1 (default); Recommended Values: -4, -3, -2, -1.5, -1, -0.5\n",
            "gap_extension_penalty = -0.2 (default); Recommended Values: -1, -0.8, -0.5, -0.3, -0.2, -0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saving_add =  \"/content/\"\n",
        "seqs_path = \"Test2.fasta\"\n",
        "scoring = MODELS_LIST[0] # \"ProtT5\"\n",
        "alignment_type = ALIGNMENT_TYPES[0] # \"Global-regular\"\n",
        "gap_penalty = -1\n",
        "gap_extension_penalty = -0.2\n",
        "\n",
        "alignment_file_TXT(saving_add = saving_add , seqs_path = seqs_path, scoring = scoring, alignment_type = alignment_type,\n",
        "                      gap_penalty = gap_penalty, gap_extension_penalty = gap_extension_penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glng92tRG5ZZ",
        "outputId": "f876fd26-98b1-4520-c2ed-337e2758f0ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProtT5 Initialize : \n",
            "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n",
            "Alignment Computation is Done!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Dq9u86RyHKk_",
        "8LCdgRFBFWW6",
        "8ctdjBhTGi9z",
        "wiNZQ2SQJM54",
        "9IMkkTPWsaF0"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}